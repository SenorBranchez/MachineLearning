\documentclass[12pt]{article}

\usepackage{pdfpages}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{tikz}
\usetikzlibrary{automata,positioning}

\usepackage{algpseudocode}
\algnewcommand{\LineComment}[1]{\State \(\triangleright\) #1}
\usepackage{fancyhdr}
\pagestyle{fancy}
\renewcommand\headrule{}
\fancyhead[L]{Gregor Bankhamer 1220843 Wolfgang Kremser 1222223 \\Wang Michelle Yih-chyan 1641792}
\fancyhead[R]{Sheet A+B}

\newcommand{\RN}[1]{%
  \textup{\uppercase\expandafter{\romannumeral#1}}%
}

\DeclareMathOperator{\ggT}{ggT}


\setlength\parindent{0pt}

\begin{document}
\section*{Sheet A}
\section*{Excercise 4}
Assume that $H$ is agnostic PAC learnable and consider an arbitrary distribution $D$ over $X \times Y$. \\\\
Furthermore assume realizablity, i.e for $(x,y) \sim D$ there exist functions $f$ and $h^* \in H$ such that $P[f(x) = y] = 1 $. \\\\
Consider $L_D(h)$, defined as follows for any arbitrary $h \in H$
\begin{align*}
	L_D(h) = P_{(x,y) \sim D}[h(x) \neq y] &\overset{real.}{=}  P_{(x,y) \sim D}[h(x) \neq f(x)] \\
	&\overset{real.}{=}  P_{x \sim D|x}[h(x) \neq f(x)]  = L_{D|x,f}(h)
\end{align*}
Since the $y$ under realizability remain the same for each $x$ it is enough to consider $D|x$ together with a labeling function $f$. Also since realizability it follows that $h^* \in H$ and therefore $L_D(h*) = L_{D|x,f}(h*) = 0$\\\\
Now since we assume agnostic PAC learnability: There must be an algorithm $A$ and $m(\delta,\epsilon)$ such that with probability $1-\delta$ for every $m \geq m(\delta,\epsilon)$ holds 
\begin{align*}
	L_{D|x,f}(h)= L_D(h) &\leq min_{h' \in H}(L_D(h')) + \epsilon \\
	&\overset{h^* \in H}{\leq}  L_D(h^*) + \epsilon = 0 + \epsilon =  \epsilon
\end{align*}
This means that $H$ together with $D|x$ is PAC learnable using algorithm $A$ and $m\geq m(\epsilon,\delta)$ of the agnostic PAC learner.
\section*{Excercise 5}
Consider the following ERM algorithm $A$: Given sample set $S$ it outputs $A(S)=h_{r_s}$ such that $r_s = \max_{(x,y) \in S, y=1}(|x|)$. \\\\
Let $H_B=\{h_r: L_{D,f}(h_r) > \epsilon\}$ for some $0 < \epsilon < 1$. Be the set of all possible bad hypothesis. \\
Furthermore for every $S$ let $h_{r_s}$ denote the hypothesis output by $A(S)$ and since realizability holds, let denote $h_{r*} \in H$ denote an optimal hypothesis. Then consider the set of bad samples $S_B$
\begin{align*}
	S_B &= \{S|x : h_{r_s} \in H_B\} \\
	&= \{S|x: L_{D,f}(h_{r_s}) > \epsilon\} \\
	&= \{S|x: P[h_{r_s}(x) \neq h_{r*}(x)] > \epsilon\} \\
	&= \{S|x: P[h_{r_s}(x) = 1 \land h_{r*}(x) = 0] +  P[h_{r_s}(x) = 0 \land h_{r*}(x) = 1] > \epsilon \}\\
	&= \{S|x: P[r* < |x| \leq r_s] + P[r_s < |x| \leq r*] > \epsilon \}
\end{align*}
However since $r_s$ is chosen such that it is the minimum radius containing all samples with $y=1$. It follows that $r_s \leq r*$. Otherwise $h_r*$ would be making at least one mistake. Therefore $P[r* < |x| \leq r_s] = 0$ and
\begin{align*}
	S_B &= \{S|x:  P[r_s < |x| \leq r*] > \epsilon \}
\end{align*}
Now we consider two cases\\
\textbf{1.Case: }$S_B$ has $r_m = \max_{S \in S_B} (r_s)$.\\
Clearly since $r_m$ results from the set $S_B$ it follows that $Pr[r_m < |x| \leq r^*] > \epsilon$. Additionally since making the radius smaller can only generate a larger or equal error we can express $S_B$ as follows
\begin{equation*}
	S_B = \{S|x: r_s \leq r_m\}
\end{equation*}
And finally we can compute
\begin{align*}
	D^m(S_B) &= D^m \{S|x : r_s \leq r_m\}\\
	&= D^m \{S|x : \forall x \in S|x: (|x| > r^*) \lor (|x| \leq r_m)\} \\
	&\overset{i.i.d}{=} \prod_{i=1}^{m} D^m\{x: \neg (r_m < |x|\leq r*)\} \\
	&< \prod_{i=1}^{m} (1- P[r_m < |x| \leq r*]) \leq (1- \epsilon)^m < e^{-\epsilon m}
\end{align*}
\textbf{2.Case:} $S_B$ only has an supremum  $r_i = \sup_{S \in S_B}(r_s)$.\\
Since $\forall S \in S_B:  P[r_s < |x| \leq r*] > \epsilon$ it follows $P[r_i \leq |x| \leq r*] > \epsilon$. Similar we can express $S_B$ now as 
\begin{equation*}
	S_B = \{S|x: r_s <r_i\}
\end{equation*}
And compute
\begin{align*}
	D^m(S_B) &= D^m \{S|x : r_s < r_i\}\\
	&= D^m \{S|x : \forall x \in S|x: (|x| > r^*) \lor (|x| < r_i)\} \\
	&\overset{i.i.d}{=} \prod_{i=1}^{m} D^m\{x: \neg (r_i \leq |x|\leq r*)\} \\
	&< \prod_{i=1}^{m} (1- P[r_i \leq |x| \leq r*]) \leq (1- \epsilon)^m < e^{-\epsilon m}
\end{align*}
\textbf{Both cases}\\
For $e^{-\epsilon m} \leq \delta$ we need
\begin{equation*}
	m \geq \frac{\log(1/\delta)}{\epsilon}
\end{equation*}
Therefore we have given an algorithm $A$ and a funtion $m_H(\epsilon,\delta)$ such that $H$ is PAC learnable returning a hypothesis that w.p $1-\delta$ holds $L_D,f(h) \leq \epsilon$.
\section*{Sheet B}
\section*{Exercise 1}
\subsection*{(2) $\rightarrow$ (1)}
Let $ D,A,L_D$ be arbitrary as described in the specification. \\\\
Since $L_D(h)  = E[\ell(h,z)]$ and $\ell$ ranges into $[0,1]$ it is cleary positiv. We can therefore use Markov's Inequality and get
\begin{equation*}
	P[L_D(A(S)) \geq \epsilon] \leq \frac{E[L_D(A(S))]}{\epsilon}
\end{equation*}
No consider the following limit definition:
\begin{align*}
	&\lim_{n \rightarrow \infty}  f(n)  = a \Leftrightarrow\\
		&\forall \epsilon> 0 :\exists \delta>0 :\forall x > \delta : |f(x) - a| < \epsilon
\end{align*}
Assume that $\lim_{n \rightarrow \infty} E[L_D(A(S))] = 0$ this is equivalent to
\begin{align*}
	\forall \delta> 0 :\exists m(\delta) :\forall m \geq m( \delta) : E[L_D(A(S))] \leq \delta
\end{align*}
When applying above limit definition and naming variables appropriatly. Note that we write $m(\delta)$ since it, among others, it depends on $\delta$.\\
Additionally since $E[L_D(A(S)))]$ is positive we can ommit the absolute value (and $a=0$). \\ \\
Let $\epsilon > 0$ be arbitrary but fixed.
Since above statement holds $\forall \delta>0$, we can also write.
\begin{align*}
	\forall \delta> 0 :\exists m (\epsilon, \delta) :\forall m \geq m( \epsilon,\delta) : E[L_D(A(S))] \leq \underline{\delta \epsilon}
\end{align*}
when inserting above application of Markov's inequality this \textit{implies}
\begin{align*}
	\forall \delta> 0 :\exists m (\epsilon, \delta) :\forall m \geq m( \epsilon, \delta ) : P[L_D(A(S)) \geq \epsilon] \leq \delta
\end{align*}
when "converting" the arbitrary fixing of $\epsilon,D,A,L_D$ into all-quantifiers we get the desired statement of (1).
\subsection*{(2) $\rightarrow$ (1)}
Because $0 \leq L_D(A(S)) \leq 1$  and  $0 \leq P_{S~D^m}[L_D(A(S)) \leq \epsilon]$<1\\
\begin{align*}
	E_{S~D^m}[L_D(A(S))] &\leq P_{S~D^m}[L_D(A(S))\geq \epsilon]* L_D(A(S))\\
	&+ P_{S~D^m}[L_D(A(S))<\epsilon] * L_D(A(S))
\end{align*}
$= \delta * 1 + \epsilon * 1 = 2 * max\{\delta, \epsilon\}$\\
$\forall m \geq m(2 * max \{ \delta, \epsilon \}), E_{S~D^m(L_D(A(S))} \leq 2 * max\{\delta, \epsilon\}$\\
$\lim_{n \rightarrow \infty} E_{S~D^m}(L_D(A(S))) = 0$

\end{document}
